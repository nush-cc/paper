"""
Bayesian Optimization for MODWT-MoE Hyperparameters
é‡å° Loss æ¬Šé‡å’Œé—œéµæ¶æ§‹åƒæ•¸é€²è¡Œå„ªåŒ–
"""

import numpy as np
import torch
import pandas as pd
from skopt import gp_minimize
from skopt.space import Real, Integer, Categorical
from skopt.plots import plot_convergence, plot_objective
from skopt.utils import use_named_args
import matplotlib.pyplot as plt
import warnings
from tqdm import tqdm
warnings.filterwarnings('ignore')

# å¼•å…¥ä½ çš„ä¸»ç¨‹å¼æ¨¡çµ„
from main import (
    prepare_modwt_data,
    train_modwt_moe,
    evaluate,
    MODWTMoE,
    CombinedLoss,
    DEVICE
)

# ==================== é…ç½® ====================
optimization_pbar = None
RANDOM_SEEDS = [42, 123, 456]  # æ¯çµ„åƒæ•¸æ¸¬è©¦ 3 æ¬¡
NUM_EPOCHS = 40  # ç¸®çŸ­è¨“ç·´åŠ é€Ÿå„ªåŒ–
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print("ğŸš€ è²è‘‰æ–¯å„ªåŒ–åˆå§‹åŒ–")
print(f"   Device: {DEVICE}")
print(f"   æ¯çµ„åƒæ•¸æ¸¬è©¦ç¨®å­: {RANDOM_SEEDS}")
print(f"   è¨“ç·´ Epochs: {NUM_EPOCHS}")

# ==================== è¼‰å…¥è³‡æ–™ï¼ˆåªè¼‰å…¥ä¸€æ¬¡ï¼‰====================
print("\nğŸ“‚ è¼‰å…¥è³‡æ–™...")
df = pd.read_csv("../dataset/USD_TWD.csv")
df["Date"] = pd.to_datetime(df["Date"])
df = df.sort_values("Date").reset_index(drop=True)
print(f"âœ… è¼‰å…¥ {len(df)} å¤©è³‡æ–™")

# ==================== æœç´¢ç©ºé–“å®šç¾© ====================
search_space = [
    Real(0.10, 0.35, name='beta'),         # Direction Loss æ¬Šé‡
    Real(0.03, 0.15, name='gamma'),        # Diversity Loss æ¬Šé‡
    Real(5e-4, 3e-3, name='lr', prior='log-uniform'),  # å­¸ç¿’ç‡
    Integer(64, 128, name='expert2_hidden')  # Expert 2 å®¹é‡
]

print("\nğŸ” æœç´¢ç©ºé–“:")
for param in search_space:
    if hasattr(param, 'prior'):
        print(f"   {param.name}: [{param.bounds[0]:.5f}, {param.bounds[1]:.5f}] (log-uniform)")
    else:
        print(f"   {param.name}: {param.bounds}")

# ==================== ç›®æ¨™å‡½æ•¸ ====================
@use_named_args(search_space)
def objective(beta, gamma, lr, expert2_hidden):
    """
    è¨“ç·´æ¨¡å‹ä¸¦è¿”å›ç›®æ¨™å€¼ï¼ˆè¦æœ€å°åŒ–ï¼‰
    ç›®æ¨™ = RMSE - 0.3 * Direction_Acc
    (é¼“å‹µ RMSE ä½ä¸”æ–¹å‘æº–ç¢ºåº¦é«˜)
    """

    global optimization_pbar

    print(f"\n{'='*70}")
    print(f"ğŸ§ª æ¸¬è©¦åƒæ•¸:")
    print(f"   beta={beta:.4f}, gamma={gamma:.4f}")
    print(f"   lr={lr:.6f}, expert2_hidden={expert2_hidden}")
    print(f"{'='*70}")

    expert2_hidden = int(expert2_hidden)

    rmse_list = []
    direction_list = []
    r2_list = []

    seed_pbar = tqdm(enumerate(RANDOM_SEEDS),
                     total=len(RANDOM_SEEDS),
                     desc="  éš¨æ©Ÿç¨®å­",
                     leave=False,
                     ncols=80)

    # è·‘ 3 æ¬¡ä¸åŒéš¨æ©Ÿç¨®å­
    for seed_idx, seed in seed_pbar:
        seed_pbar.set_description(f"  ç¨®å­ {seed_idx+1}/{len(RANDOM_SEEDS)}: {seed}")

        # è¨­å®šéš¨æ©Ÿç¨®å­
        torch.manual_seed(seed)
        np.random.seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(seed)

        try:
            # æº–å‚™è³‡æ–™
            train_loader, test_loader, scalers, _, _ = prepare_modwt_data(
                df,
                wavelet='sym4',
                level=4,
                lookback=30,
                forecast_horizon=1,
                train_ratio=0.8
            )

            # å»ºç«‹æ¨¡å‹ï¼ˆå‹•æ…‹æ¶æ§‹ï¼‰
            from main import TrendExpert, CyclicExpert, HighFreqExpert, GatingNetwork

            class MODWTMoEDynamic(torch.nn.Module):
                """å‹•æ…‹æ¶æ§‹çš„ MoE"""
                def __init__(self, expert2_hidden):
                    super().__init__()
                    self.expert1 = TrendExpert(input_size=1, hidden_size=32, num_layers=2, dropout=0.2)
                    self.expert2 = CyclicExpert(input_size=2, hidden_size=expert2_hidden, num_layers=2, dropout=0.3)
                    self.expert3 = HighFreqExpert(input_size=2, hidden_size=32, num_layers=2, dropout=0.4)
                    self.gating = GatingNetwork(input_size=13, hidden_size=32, num_experts=3)

                def forward(self, expert1_input, expert2_input, expert3_input):
                    from main import extract_gating_features
                    pred1 = self.expert1(expert1_input)
                    pred2 = self.expert2(expert2_input)
                    pred3 = self.expert3(expert3_input)
                    expert_preds = torch.cat([pred1, pred2, pred3], dim=1)
                    gating_features = extract_gating_features(expert1_input, expert2_input, expert3_input)
                    gating_weights = self.gating(gating_features)
                    final_pred = (expert_preds * gating_weights).sum(dim=1, keepdim=True)
                    return final_pred, expert_preds, gating_weights

            model = MODWTMoEDynamic(expert2_hidden=expert2_hidden).to(DEVICE)

            # ä½¿ç”¨ç•¶å‰åƒæ•¸
            criterion = CombinedLoss(
                huber_delta=1.0,
                alpha=1.0,      # å›ºå®š
                beta=beta,      # å„ªåŒ–
                gamma=gamma     # å„ªåŒ–
            )

            optimizer = torch.optim.Adam(
                model.parameters(),
                lr=lr,          # å„ªåŒ–
                weight_decay=1e-5
            )

            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                optimizer, mode='min', factor=0.5, patience=5
            )

            # è¨“ç·´ï¼ˆç°¡åŒ–ç‰ˆï¼‰
            from main import train_one_epoch

            best_rmse = float('inf')
            patience = 0

            epoch_pbar = tqdm(range(NUM_EPOCHS),
                              desc="    Epoch",
                              leave=False,
                              ncols=80)

            for epoch in epoch_pbar:
                train_loss, _ = train_one_epoch(
                    model, train_loader, optimizer, criterion, DEVICE
                )

                metrics, preds, targets, expert_preds, gating_weights = evaluate(
                    model, test_loader, DEVICE
                )

                scheduler.step(metrics['rmse'])

                epoch_pbar.set_postfix({
                    'loss': f'{train_loss:.4f}',
                    'rmse': f'{metrics["rmse"]:.4f}',
                    'dir': f'{metrics["direction_acc"]:.3f}'
                })

                if metrics['rmse'] < best_rmse:
                    best_rmse = metrics['rmse']
                    patience = 0
                    best_metrics = metrics
                else:
                    patience += 1

                if patience >= 8:  # æ—©åœ
                    break

            epoch_pbar.close()

            # Inverse transform
            target_scaler = scalers['cA4_trend']
            preds_original = target_scaler.inverse_transform(preds.reshape(-1, 1)).flatten()
            targets_original = target_scaler.inverse_transform(targets.reshape(-1, 1)).flatten()

            from sklearn.metrics import mean_squared_error
            rmse_original = np.sqrt(mean_squared_error(targets_original, preds_original))

            rmse_list.append(rmse_original)
            direction_list.append(best_metrics['direction_acc'])
            r2_list.append(best_metrics['r2'])

            # æ›´æ–°ç¨®å­é€²åº¦æ¢
            seed_pbar.set_postfix({
                'RMSE': f'{rmse_original:.4f}%',
                'Dir': f'{best_metrics["direction_acc"]:.3f}'
            })

            print(f"    âœ… RMSE: {rmse_original:.4f}%, Direction: {best_metrics['direction_acc']:.3f}, RÂ²: {best_metrics['r2']:.4f}")

        except Exception as e:
            print(f"    âŒ è¨“ç·´å¤±æ•—: {e}")
            return 10.0  # æ‡²ç½°å€¼

    seed_pbar.close()

    # è¨ˆç®—å¹³å‡
    avg_rmse = np.mean(rmse_list)
    avg_direction = np.mean(direction_list)
    avg_r2 = np.mean(r2_list)

    print(f"\n  ğŸ“Š å¹³å‡çµæœ:")
    print(f"     RMSE: {avg_rmse:.4f}%")
    print(f"     Direction: {avg_direction:.3f}")
    print(f"     RÂ²: {avg_r2:.4f}")

    # ç›®æ¨™å‡½æ•¸ï¼šä¸»è¦å„ªåŒ– RMSEï¼Œå…¼é¡§æ–¹å‘æº–ç¢ºåº¦
    objective_value = avg_rmse - 0.3 * avg_direction

    print(f"     ç›®æ¨™å€¼: {objective_value:.6f}")

    return objective_value

# ==================== åŸ·è¡Œå„ªåŒ– ====================
print("\n" + "="*80)
print("ğŸš€ é–‹å§‹è²è‘‰æ–¯å„ªåŒ–")
print("="*80)

optimization_pbar = tqdm(total=30, desc="è²è‘‰æ–¯å„ªåŒ–", ncols=100)

result = gp_minimize(
    func=objective,
    dimensions=search_space,
    n_calls=30,              # ç¸½å…±è©•ä¼° 30 çµ„åƒæ•¸
    n_initial_points=5,      # å‰ 5 çµ„éš¨æ©Ÿæ¢ç´¢
    initial_point_generator='sobol',
    acq_func='EI',           # Expected Improvement
    n_jobs=1,
    verbose=True,
    random_state=42
)

optimization_pbar.close()

print("\n" + "="*80)
print("âœ… å„ªåŒ–å®Œæˆï¼")
print("="*80)

# ==================== çµæœåˆ†æ ====================
print("\nğŸ“Š æœ€ä½³åƒæ•¸:")
best_params = {
    'beta': result.x[0],
    'gamma': result.x[1],
    'lr': result.x[2],
    'expert2_hidden': result.x[3]
}

for param_name, param_value in best_params.items():
    print(f"   {param_name}: {param_value}")

print(f"\nğŸ† æœ€ä½³ç›®æ¨™å€¼: {result.fun:.6f}")

# ä¼°ç®—å°æ‡‰çš„ RMSE
estimated_rmse = result.fun + 0.3 * 0.77  # å‡è¨­ direction â‰ˆ 0.77
print(f"   (ä¼°è¨ˆ RMSE â‰ˆ {estimated_rmse:.4f}%)")


# ==================== è¦–è¦ºåŒ– ====================
print("\nğŸ“Š ç¹ªè£½å„ªåŒ–éç¨‹...")

# åœ– 1: æ”¶æ–‚æ›²ç·š
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# Convergence plot
plot_convergence(result, ax=axes[0])
axes[0].set_title('Optimization Convergence', fontsize=14, fontweight='bold')
axes[0].set_xlabel('Number of Evaluations')
axes[0].set_ylabel('Objective Value (lower is better)')

# åƒæ•¸æ­·å²
iterations = range(1, len(result.func_vals) + 1)
axes[1].plot(iterations, result.func_vals, 'o-', alpha=0.6, label='Objective Value')
axes[1].axhline(y=result.fun, color='r', linestyle='--', linewidth=2, label=f'Best: {result.fun:.4f}')
axes[1].set_xlabel('Iteration', fontsize=12)
axes[1].set_ylabel('Objective Value', fontsize=12)
axes[1].set_title('Objective Value History', fontsize=14, fontweight='bold')
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.savefig('../results/bayesian_opt/convergence.png', dpi=300, bbox_inches='tight')
print("   âœ… å„²å­˜: ../results/bayesian_opt/convergence.png")
plt.close()

# åœ– 2: åƒæ•¸é‡è¦æ€§ï¼ˆéƒ¨åˆ†ä¾è³´åœ–ï¼‰
try:
    from skopt.plots import plot_evaluations
    fig = plt.figure(figsize=(12, 10))
    plot_evaluations(result)
    plt.tight_layout()
    plt.savefig('../results/bayesian_opt/evaluations.png', dpi=300, bbox_inches='tight')
    print("   âœ… å„²å­˜: ../results/bayesian_opt/evaluations.png")
    plt.close()
except:
    print("   âš ï¸ ç„¡æ³•ç¹ªè£½ evaluations åœ–ï¼ˆå¯èƒ½éœ€è¦æ›´å¤šè¿­ä»£ï¼‰")

# ==================== å„²å­˜çµæœ ====================
results_df = pd.DataFrame({
    'iteration': range(1, len(result.func_vals) + 1),
    'objective': result.func_vals,
    'beta': [x[0] for x in result.x_iters],
    'gamma': [x[1] for x in result.x_iters],
    'lr': [x[2] for x in result.x_iters],
    'expert2_hidden': [x[3] for x in result.x_iters]
})

results_df.to_csv('../results/bayesian_opt/optimization_history.csv', index=False)
print("\nğŸ’¾ å„ªåŒ–æ­·å²å·²å„²å­˜: ../results/bayesian_opt/optimization_history.csv")

# å„²å­˜æœ€ä½³åƒæ•¸
with open('../results/bayesian_opt/best_params.txt', 'w') as f:
    f.write("="*50 + "\n")
    f.write("è²è‘‰æ–¯å„ªåŒ– - æœ€ä½³åƒæ•¸\n")
    f.write("="*50 + "\n\n")
    for param_name, param_value in best_params.items():
        f.write(f"{param_name}: {param_value}\n")
    f.write(f"\næœ€ä½³ç›®æ¨™å€¼: {result.fun:.6f}\n")
    f.write(f"ä¼°è¨ˆ RMSE: {estimated_rmse:.4f}%\n")

print("ğŸ’¾ æœ€ä½³åƒæ•¸å·²å„²å­˜: ../results/bayesian_opt/best_params.txt")

print("\n" + "="*80)
print("âœ… è²è‘‰æ–¯å„ªåŒ–å®Œæˆï¼")
print("="*80)