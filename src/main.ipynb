{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-13T07:58:08.494337Z",
     "start_time": "2025-11-13T07:57:41.317862Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T07:58:08.737047Z",
     "start_time": "2025-11-13T07:58:08.512329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import modwtpy\n",
    "try:\n",
    "    from modwt import modwt, imodwt, modwtmra\n",
    "    print(\"âœ… modwtpy imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"âŒ modwtpy not found. Please install it:\")\n",
    "    print(\"   pip install modwtpy\")\n",
    "    print(\"   or: pip install git+https://github.com/pistonly/modwtpy.git\")\n",
    "    raise"
   ],
   "id": "e54c29dda12e43f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… modwtpy imported successfully\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T07:58:09.578368Z",
     "start_time": "2025-11-13T07:58:09.404620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==================== Configuration ====================\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "print(f\"ğŸ”§ Device: {DEVICE}\")\n",
    "print(f\"ğŸ”§ Random Seed: {SEED}\")\n",
    "print(\"âœ… Setup Complete!\\n\")"
   ],
   "id": "601e7b8485e72106",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Device: cuda\n",
      "ğŸ”§ Random Seed: 42\n",
      "âœ… Setup Complete!\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T07:58:09.655391Z",
     "start_time": "2025-11-13T07:58:09.648517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==================== MODWT Decomposer with MRA ====================\n",
    "class MODWTDecomposer:\n",
    "    \"\"\"Perform MODWT decomposition using modwtpy with MRA\"\"\"\n",
    "\n",
    "    def __init__(self, wavelet='db4', level=4):\n",
    "        self.wavelet = wavelet\n",
    "        self.level = level\n",
    "        self.components_names = None\n",
    "\n",
    "    def decompose(self, signal, use_mra=True):\n",
    "        \"\"\"\n",
    "        Decompose signal using MODWT\n",
    "\n",
    "        Args:\n",
    "            signal: 1D numpy array\n",
    "            use_mra: if True, return MRA components (recommended)\n",
    "\n",
    "        Returns:\n",
    "            components: dict with keys like 'cA4_trend', 'cD4', 'cD3', etc.\n",
    "        \"\"\"\n",
    "        print(f\"   Decomposing with modwtpy (wavelet={self.wavelet}, level={self.level}, MRA={use_mra})...\")\n",
    "\n",
    "        # Perform MODWT\n",
    "        try:\n",
    "            w = modwt(signal, self.wavelet, self.level)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ MODWT failed: {e}\")\n",
    "            print(f\"   Try using 'haar', 'db2', 'db4', or 'sym4'\")\n",
    "            raise\n",
    "\n",
    "        # Use MRA for better interpretability\n",
    "        if use_mra:\n",
    "            mra = modwtmra(w, self.wavelet)\n",
    "            # mra shape: [level+1, N]\n",
    "            # mra[0] = D1, mra[1] = D2, ..., mra[-1] = S_J\n",
    "\n",
    "            components = {}\n",
    "            # Details (å¾ç´°åˆ°ç²—)\n",
    "            for i in range(self.level):\n",
    "                components[f'cD{i+1}'] = mra[i]\n",
    "\n",
    "            # Approximation (trend)\n",
    "            components[f'cA{self.level}_trend'] = mra[-1]\n",
    "\n",
    "        else:\n",
    "            # Use raw MODWT coefficients\n",
    "            components = {}\n",
    "            # w shape: [level+1, N]\n",
    "            # w[0] = w1, w[1] = w2, ..., w[-1] = v_J\n",
    "\n",
    "            for i in range(self.level):\n",
    "                components[f'cD{i+1}'] = w[i]\n",
    "            components[f'cA{self.level}_trend'] = w[-1]\n",
    "\n",
    "        self.components_names = list(components.keys())\n",
    "\n",
    "        # Verify reconstruction (only for MRA)\n",
    "        if use_mra:\n",
    "            reconstructed = sum(components.values())\n",
    "            recon_error = np.max(np.abs(signal - reconstructed))\n",
    "        else:\n",
    "            try:\n",
    "                reconstructed = imodwt(w, self.wavelet)\n",
    "                recon_error = np.max(np.abs(signal - reconstructed[:len(signal)]))\n",
    "            except:\n",
    "                recon_error = np.nan\n",
    "\n",
    "        print(f\"   âœ… MODWT Decomposition Complete\")\n",
    "        print(f\"      Wavelet: {self.wavelet}, Level: {self.level}\")\n",
    "        print(f\"      Components: {self.components_names}\")\n",
    "        print(f\"      Signal length: {len(signal)}\")\n",
    "\n",
    "        if not np.isnan(recon_error):\n",
    "            print(f\"      Reconstruction Error: {recon_error:.10f}\")\n",
    "\n",
    "        return components\n",
    "\n",
    "    def get_component_energies(self, components):\n",
    "        \"\"\"Calculate energy percentage for each component\"\"\"\n",
    "        total_energy = sum(np.sum(comp**2) for comp in components.values())\n",
    "        energies = {}\n",
    "        for name, comp in components.items():\n",
    "            energy_pct = np.sum(comp**2) / total_energy * 100\n",
    "            energies[name] = energy_pct\n",
    "        return energies"
   ],
   "id": "25585e816ce84bdd",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T07:58:09.712555Z",
     "start_time": "2025-11-13T07:58:09.706603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==================== Dataset ====================\n",
    "class MODWTVolatilityDataset(Dataset):\n",
    "    \"\"\"Dataset for MODWT-MoE model\"\"\"\n",
    "\n",
    "    def __init__(self, components_dict, target, window=30, forecast_horizon=1):\n",
    "        self.window = window\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "\n",
    "        self.expert1_data = []  # cA4\n",
    "        self.expert2_data = []  # cD4, cD3 stacked\n",
    "        self.expert3_data = []  # cD2, cD1 stacked\n",
    "        self.targets = []\n",
    "\n",
    "        # å–å¾—å„å€‹æˆåˆ†\n",
    "        cA4 = components_dict['cA4_trend']\n",
    "        cD4 = components_dict['cD4']\n",
    "        cD3 = components_dict['cD3']\n",
    "        cD2 = components_dict['cD2']\n",
    "        cD1 = components_dict['cD1']\n",
    "\n",
    "        # ç¢ºä¿æ‰€æœ‰ component é•·åº¦ä¸€è‡´\n",
    "        min_len = min(len(cA4), len(cD4), len(cD3), len(cD2), len(cD1), len(target))\n",
    "        cA4 = cA4[:min_len]\n",
    "        cD4 = cD4[:min_len]\n",
    "        cD3 = cD3[:min_len]\n",
    "        cD2 = cD2[:min_len]\n",
    "        cD1 = cD1[:min_len]\n",
    "        target = target[:min_len]\n",
    "\n",
    "        # Create sliding windows\n",
    "        for i in range(len(cA4) - window - forecast_horizon + 1):\n",
    "            self.expert1_data.append(cA4[i:i+window])\n",
    "\n",
    "            expert2_window = np.stack([cD4[i:i+window], cD3[i:i+window]], axis=1)\n",
    "            self.expert2_data.append(expert2_window)\n",
    "\n",
    "            expert3_window = np.stack([cD2[i:i+window], cD1[i:i+window]], axis=1)\n",
    "            self.expert3_data.append(expert3_window)\n",
    "\n",
    "            self.targets.append(target[i + window + forecast_horizon - 1])\n",
    "\n",
    "        # Convert to tensors\n",
    "        self.expert1_data = torch.FloatTensor(np.array(self.expert1_data)).unsqueeze(-1)\n",
    "        self.expert2_data = torch.FloatTensor(np.array(self.expert2_data))\n",
    "        self.expert3_data = torch.FloatTensor(np.array(self.expert3_data))\n",
    "        self.targets = torch.FloatTensor(np.array(self.targets)).unsqueeze(-1)\n",
    "\n",
    "        print(f\"   âœ… Dataset Created: {len(self.targets)} samples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'expert1': self.expert1_data[idx],\n",
    "            'expert2': self.expert2_data[idx],\n",
    "            'expert3': self.expert3_data[idx],\n",
    "            'target': self.targets[idx]\n",
    "        }"
   ],
   "id": "e47a83cddff01566",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T07:58:09.741005Z",
     "start_time": "2025-11-13T07:58:09.734598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==================== Expert Networks ====================\n",
    "class TrendExpert(nn.Module):\n",
    "    \"\"\"Expert 1: Trend prediction (cA4)\"\"\"\n",
    "\n",
    "    def __init__(self, input_size=1, hidden_size=32, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers,\n",
    "                          batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.dropout(out)\n",
    "        return self.fc(out)\n",
    "\n",
    "\n",
    "class CyclicExpert(nn.Module):\n",
    "    \"\"\"Expert 2: Cyclic prediction (cD4 + cD3)\"\"\"\n",
    "\n",
    "    def __init__(self, input_size=2, hidden_size=64, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers,\n",
    "                          batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        attention_weights = self.attention(out)\n",
    "        attention_weights = torch.softmax(attention_weights, dim=1)\n",
    "        out = torch.sum(out * attention_weights, dim=1)\n",
    "        out = self.dropout(out)\n",
    "        return self.fc(out)\n",
    "\n",
    "\n",
    "class HighFreqExpert(nn.Module):\n",
    "    \"\"\"Expert 3: High-frequency/noise (cD2 + cD1)\"\"\"\n",
    "\n",
    "    def __init__(self, input_size=2, hidden_size=32, num_layers=2, dropout=0.4):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers,\n",
    "                          batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.dropout(out)\n",
    "        return self.fc(out)"
   ],
   "id": "633de7e27da770ad",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T07:58:09.752794Z",
     "start_time": "2025-11-13T07:58:09.748509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==================== Enhanced Gating Network ====================\n",
    "class GatingNetwork(nn.Module):\n",
    "    \"\"\"Enhanced Gating network with richer features\"\"\"\n",
    "\n",
    "    def __init__(self, input_size=5, hidden_size=128, num_experts=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size // 2),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, num_experts)\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, combined_input):\n",
    "        logits = self.fc(combined_input)\n",
    "        return self.softmax(logits)"
   ],
   "id": "4811fce29b6036cb",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T07:58:09.763698Z",
     "start_time": "2025-11-13T07:58:09.756643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==================== MoE Model ====================\n",
    "class MODWTMoE(nn.Module):\n",
    "    \"\"\"Complete MODWT-MoE model\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.expert1 = TrendExpert()\n",
    "        self.expert2 = CyclicExpert()\n",
    "        self.expert3 = HighFreqExpert()\n",
    "        self.gating = GatingNetwork()\n",
    "\n",
    "    def forward(self, expert1_input, expert2_input, expert3_input):\n",
    "        # Expert predictions\n",
    "        pred1 = self.expert1(expert1_input)\n",
    "        pred2 = self.expert2(expert2_input)\n",
    "        pred3 = self.expert3(expert3_input)\n",
    "\n",
    "        # Gating input: last timestep features from all experts\n",
    "        e1_last = expert1_input[:, -1, :]  # [batch, 1]\n",
    "        e2_last = expert2_input[:, -1, :]  # [batch, 2]\n",
    "        e3_last = expert3_input[:, -1, :]  # [batch, 2]\n",
    "\n",
    "        # Flatten if needed\n",
    "        if e1_last.dim() > 2:\n",
    "            e1_last = e1_last.squeeze(-1)\n",
    "        if e1_last.dim() == 1:\n",
    "            e1_last = e1_last.unsqueeze(-1)\n",
    "\n",
    "        gate_input = torch.cat([e1_last, e2_last, e3_last], dim=1)  # [batch, 5]\n",
    "\n",
    "        # Gating weights\n",
    "        weights = self.gating(gate_input)\n",
    "\n",
    "        # Weighted combination\n",
    "        predictions = torch.stack([pred1, pred2, pred3], dim=2)\n",
    "        output = torch.sum(predictions * weights.unsqueeze(1), dim=2)\n",
    "\n",
    "        return output, weights, predictions.squeeze(1)"
   ],
   "id": "11607e63d4e1fe75",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T07:58:09.778665Z",
     "start_time": "2025-11-13T07:58:09.770703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==================== Data Preparation with Centering ====================\n",
    "def prepare_modwt_data(df, vol_window=7, lookback=30, forecast_horizon=1,\n",
    "                       wavelet='db4', level=4,\n",
    "                       train_ratio=0.80, batch_size=32,  # ç§»é™¤ val_ratio\n",
    "                       use_robust_scaler=False):\n",
    "    \"\"\"\n",
    "    æº–å‚™ MODWT è³‡æ–™ï¼Œä½¿ç”¨å»ä¸­å¿ƒåŒ–è™•ç†\n",
    "    ä¿®æ”¹ç‚º 80/20 Train/Test åˆ‡åˆ†\n",
    "\n",
    "    é—œéµæ”¹é€²ï¼š\n",
    "    1. å…ˆåˆ‡åˆ†æ™‚é–“åºåˆ—\n",
    "    2. å°æ¯å€‹å­é›†**å»ä¸­å¿ƒåŒ–**ï¼ˆæ¸›å»å‡å€¼ï¼‰\n",
    "    3. åˆ†åˆ¥åš MODWT åˆ†è§£ï¼ˆä½¿ç”¨ MRAï¼‰\n",
    "    4. é€™æ¨£ç¢ºä¿èƒ½é‡åˆ†å¸ƒåˆç†ï¼Œä¸”ç„¡è³‡æ–™æ´©æ¼\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ”§ Data Preparation with MODWT + Centering (Train/Test Split)\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Step 1: è¨ˆç®—æ³¢å‹•ç‡\n",
    "    print(\"\\nğŸ“Š Step 1: Calculate volatility...\")\n",
    "    df['log_return'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "    df['Volatility'] = df['log_return'].rolling(window=vol_window).std() * np.sqrt(252) * 100\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    volatility = df['Volatility'].values\n",
    "\n",
    "    print(f\"   âœ… Volatility calculated: {len(volatility)} samples\")\n",
    "    print(f\"      Mean: {volatility.mean():.4f}%\")\n",
    "    print(f\"      Std: {volatility.std():.4f}%\")\n",
    "    print(f\"      Range: [{volatility.min():.4f}, {volatility.max():.4f}]\")\n",
    "\n",
    "    # Step 2: æ™‚é–“åºåˆ—åˆ‡åˆ† (80/20)\n",
    "    print(\"\\nğŸ“Š Step 2: Time-series split (80% Train / 20% Test)...\")\n",
    "    total_len = len(volatility)\n",
    "    train_split_idx = int(total_len * train_ratio)\n",
    "\n",
    "    print(f\"   Total samples: {total_len}\")\n",
    "    print(f\"   Train: 0 to {train_split_idx} ({train_split_idx} samples, {train_ratio*100:.0f}%)\")\n",
    "    print(f\"   Test: {train_split_idx} to {total_len} ({total_len - train_split_idx} samples, {(1-train_ratio)*100:.0f}%)\")\n",
    "\n",
    "    train_volatility = volatility[:train_split_idx]\n",
    "    test_volatility = volatility[train_split_idx:]\n",
    "\n",
    "    # Step 3: å»ä¸­å¿ƒåŒ–ï¼ˆé—œéµæ­¥é©Ÿï¼ï¼‰\n",
    "    print(\"\\nğŸ“Š Step 3: Centering (remove mean)...\")\n",
    "\n",
    "    # åªç”¨ train çš„å‡å€¼\n",
    "    train_mean = train_volatility.mean()\n",
    "\n",
    "    train_volatility_centered = train_volatility - train_mean\n",
    "    test_volatility_centered = test_volatility - train_mean\n",
    "\n",
    "    print(f\"   âœ… Train mean: {train_mean:.4f}% (will be subtracted)\")\n",
    "    print(f\"      Train centered: mean={train_volatility_centered.mean():.6f}, std={train_volatility_centered.std():.4f}\")\n",
    "    print(f\"      Test centered: mean={test_volatility_centered.mean():.6f}, std={test_volatility_centered.std():.4f}\")\n",
    "\n",
    "    # Step 4: åˆ†åˆ¥å°æ¯å€‹å­é›†é€²è¡Œ MODWT åˆ†è§£ï¼ˆä½¿ç”¨ MRAï¼‰\n",
    "    print(\"\\nğŸ“Š Step 4: Separate MODWT decomposition with MRA...\")\n",
    "    decomposer = MODWTDecomposer(wavelet=wavelet, level=level)\n",
    "\n",
    "    print(\"\\n   ğŸ”¹ Decomposing TRAIN set...\")\n",
    "    train_components = decomposer.decompose(train_volatility_centered, use_mra=True)\n",
    "    train_energies = decomposer.get_component_energies(train_components)\n",
    "\n",
    "    print(\"\\n   ğŸ”¹ Decomposing TEST set...\")\n",
    "    test_components = decomposer.decompose(test_volatility_centered, use_mra=True)\n",
    "\n",
    "    print(\"\\nğŸ“Š Component Energies (Train set - After Centering):\")\n",
    "    for name, energy in train_energies.items():\n",
    "        print(f\"   {name}: {energy:.2f}%\")\n",
    "\n",
    "    # Step 5: Scaling\n",
    "    print(\"\\nğŸ“Š Step 5: Scaling components...\")\n",
    "    scalers = {}\n",
    "    train_components_scaled = {}\n",
    "    test_components_scaled = {}\n",
    "\n",
    "    # é¸æ“‡ Scaler\n",
    "    if use_robust_scaler:\n",
    "        print(\"   Using RobustScaler (better for outliers)\")\n",
    "        ScalerClass = RobustScaler\n",
    "    else:\n",
    "        print(\"   Using StandardScaler\")\n",
    "        ScalerClass = StandardScaler\n",
    "\n",
    "    for name in train_components.keys():\n",
    "        scaler = ScalerClass()\n",
    "\n",
    "        # åªç”¨ train è³‡æ–™ä¾† fit\n",
    "        train_components_scaled[name] = scaler.fit_transform(\n",
    "            train_components[name].reshape(-1, 1)\n",
    "        ).flatten()\n",
    "\n",
    "        # Transform test\n",
    "        test_components_scaled[name] = scaler.transform(\n",
    "            test_components[name].reshape(-1, 1)\n",
    "        ).flatten()\n",
    "\n",
    "        scalers[name] = scaler\n",
    "\n",
    "        if isinstance(scaler, StandardScaler):\n",
    "            print(f\"   âœ… {name}: Mean={scaler.mean_[0]:.6f}, Std={scaler.scale_[0]:.6f}\")\n",
    "        else:\n",
    "            print(f\"   âœ… {name}: Median={scaler.center_[0]:.6f}, IQR={scaler.scale_[0]:.6f}\")\n",
    "\n",
    "    # Scale targets (ç”¨å»ä¸­å¿ƒåŒ–çš„æ•¸æ“š)\n",
    "    target_scaler = ScalerClass()\n",
    "    train_target_scaled = target_scaler.fit_transform(train_volatility_centered.reshape(-1, 1)).flatten()\n",
    "    test_target_scaled = target_scaler.transform(test_volatility_centered.reshape(-1, 1)).flatten()\n",
    "    scalers['target'] = target_scaler\n",
    "\n",
    "    # ä¿å­˜å‡å€¼ç”¨æ–¼é‚„åŸ\n",
    "    scalers['volatility_mean'] = train_mean\n",
    "\n",
    "    # Step 6: å»ºç«‹ Dataset å’Œ DataLoader\n",
    "    print(\"\\nğŸ“Š Step 6: Create datasets...\")\n",
    "\n",
    "    train_dataset = MODWTVolatilityDataset(\n",
    "        train_components_scaled, train_target_scaled,\n",
    "        window=lookback, forecast_horizon=forecast_horizon\n",
    "    )\n",
    "\n",
    "    test_dataset = MODWTVolatilityDataset(\n",
    "        test_components_scaled, test_target_scaled,\n",
    "        window=lookback, forecast_horizon=forecast_horizon\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(f\"\\nâœ… Data preparation complete!\")\n",
    "    print(f\"   Train batches: {len(train_loader)}\")\n",
    "    print(f\"   Test batches: {len(test_loader)}\")\n",
    "\n",
    "    return train_loader, test_loader, scalers, train_components, train_energies"
   ],
   "id": "d6925e44facc08c1",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T07:58:09.786081Z",
     "start_time": "2025-11-13T07:58:09.781624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==================== Huber Loss ====================\n",
    "class HuberLoss(nn.Module):\n",
    "    \"\"\"Huber Loss for robust training\"\"\"\n",
    "\n",
    "    def __init__(self, delta=1.0):\n",
    "        super().__init__()\n",
    "        self.delta = delta\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        error = torch.abs(pred - target)\n",
    "        quadratic = torch.min(error, torch.tensor(self.delta, device=error.device))\n",
    "        linear = error - quadratic\n",
    "        loss = 0.5 * quadratic**2 + self.delta * linear\n",
    "        return loss.mean()"
   ],
   "id": "4413e8f853571abb",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T07:58:09.797333Z",
     "start_time": "2025-11-13T07:58:09.789465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==================== Training Function ====================\n",
    "def train_modwt_moe(train_loader, test_loader, num_epochs=100,\n",
    "                    lr=0.001, delta=1.0, device=DEVICE):\n",
    "    \"\"\"è¨“ç·´ MODWT-MoE æ¨¡å‹ (ç„¡ early stopping ç‰ˆæœ¬)\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸš€ Training MODWT-MoE Model (80/20 Split)\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    model = MODWTMoE().to(device)\n",
    "    criterion = HuberLoss(delta=delta)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5\n",
    "    )\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'test_loss': [],\n",
    "        'train_rmse': [],\n",
    "        'train_mae': [],\n",
    "        'epochs': []\n",
    "    }\n",
    "\n",
    "    best_train_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        train_preds = []\n",
    "        train_targets = []\n",
    "\n",
    "        for batch in train_loader:\n",
    "            e1 = batch['expert1'].to(device)\n",
    "            e2 = batch['expert2'].to(device)\n",
    "            e3 = batch['expert3'].to(device)\n",
    "            target = batch['target'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output, weights, expert_preds = model(e1, e2, e3)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            train_preds.append(output.detach().cpu().numpy())\n",
    "            train_targets.append(target.cpu().numpy())\n",
    "\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        train_preds = np.concatenate(train_preds, axis=0)\n",
    "        train_targets = np.concatenate(train_targets, axis=0)\n",
    "\n",
    "        train_rmse = np.sqrt(mean_squared_error(train_targets, train_preds))\n",
    "        train_mae = mean_absolute_error(train_targets, train_preds)\n",
    "\n",
    "        # Test Phase (åƒ…ç”¨æ–¼ç›£æ§ï¼Œä¸ä½œç‚ºåœæ­¢æ¢ä»¶)\n",
    "        model.eval()\n",
    "        test_losses = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                e1 = batch['expert1'].to(device)\n",
    "                e2 = batch['expert2'].to(device)\n",
    "                e3 = batch['expert3'].to(device)\n",
    "                target = batch['target'].to(device)\n",
    "\n",
    "                output, _, _ = model(e1, e2, e3)\n",
    "                loss = criterion(output, target)\n",
    "                test_losses.append(loss.item())\n",
    "\n",
    "        avg_test_loss = np.mean(test_losses)\n",
    "\n",
    "        scheduler.step(avg_train_loss)\n",
    "\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['test_loss'].append(avg_test_loss)\n",
    "        history['train_rmse'].append(train_rmse)\n",
    "        history['train_mae'].append(train_mae)\n",
    "        history['epochs'].append(epoch + 1)\n",
    "\n",
    "        # Print every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1:3d}/{num_epochs} | \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "                  f\"Test Loss: {avg_test_loss:.4f} | \"\n",
    "                  f\"Train RMSE: {train_rmse:.4f}\")\n",
    "\n",
    "        # ä¿å­˜æœ€ä½³æ¨¡å‹ (åŸºæ–¼ Train Loss)\n",
    "        if avg_train_loss < best_train_loss:\n",
    "            best_train_loss = avg_train_loss\n",
    "            best_epoch = epoch + 1\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "                print(f\"   âœ… New best model! Train Loss: {best_train_loss:.4f}\")\n",
    "\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"\\nâœ… Training complete! Best model from epoch {best_epoch}\")\n",
    "\n",
    "    return model, history, best_epoch"
   ],
   "id": "7cd7e939335e4531",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T07:58:09.805854Z",
     "start_time": "2025-11-13T07:58:09.801025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==================== Evaluation Function ====================\n",
    "def evaluate(model, data_loader, device):\n",
    "    \"\"\"Evaluate model on a dataset\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_expert_preds = []\n",
    "    all_gating_weights = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            e1 = batch['expert1'].to(device)\n",
    "            e2 = batch['expert2'].to(device)\n",
    "            e3 = batch['expert3'].to(device)\n",
    "            target = batch['target'].to(device)\n",
    "\n",
    "            output, weights, expert_preds = model(e1, e2, e3)\n",
    "\n",
    "            all_preds.append(output.cpu().numpy())\n",
    "            all_targets.append(target.cpu().numpy())\n",
    "            all_expert_preds.append(expert_preds.cpu().numpy())\n",
    "            all_gating_weights.append(weights.cpu().numpy())\n",
    "\n",
    "    predictions = np.concatenate(all_preds, axis=0)\n",
    "    targets = np.concatenate(all_targets, axis=0)\n",
    "    expert_preds = np.concatenate(all_expert_preds, axis=0)\n",
    "    gating_weights = np.concatenate(all_gating_weights, axis=0)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(targets, predictions))\n",
    "    mae = mean_absolute_error(targets, predictions)\n",
    "    r2 = r2_score(targets, predictions)\n",
    "\n",
    "    direction_true = np.sign(np.diff(targets.flatten()))\n",
    "    direction_pred = np.sign(np.diff(predictions.flatten()))\n",
    "    direction_acc = np.mean(direction_true == direction_pred)\n",
    "\n",
    "    metrics = {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'direction_acc': direction_acc\n",
    "    }\n",
    "\n",
    "    return metrics, predictions.flatten(), targets.flatten(), expert_preds, gating_weights"
   ],
   "id": "93d5d9649c033df2",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T07:58:09.819707Z",
     "start_time": "2025-11-13T07:58:09.809057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def walk_forward_validation(df,\n",
    "                            vol_window=7,\n",
    "                            lookback=30,\n",
    "                            forecast_horizon=1,\n",
    "                            wavelet='db4',\n",
    "                            level=4,\n",
    "                            train_window=4000,\n",
    "                            test_window=500,\n",
    "                            step_size=500,\n",
    "                            num_epochs=50,\n",
    "                            batch_size=32,\n",
    "                            lr=0.001,\n",
    "                            use_robust_scaler=False,\n",
    "                            device=DEVICE):\n",
    "    \"\"\"\n",
    "    Walk-Forward Validation for MODWT-MoE\n",
    "\n",
    "    åƒæ•¸:\n",
    "        df: åŸå§‹æ•¸æ“š DataFrame\n",
    "        train_window: æ¯å€‹ fold çš„è¨“ç·´çª—å£å¤§å° (å¤©æ•¸)\n",
    "        test_window: æ¯å€‹ fold çš„æ¸¬è©¦çª—å£å¤§å° (å¤©æ•¸)\n",
    "        step_size: æ¯æ¬¡æ»¾å‹•çš„æ­¥é•· (å¤©æ•¸)\n",
    "        num_epochs: æ¯å€‹ fold è¨“ç·´å¤šå°‘ epoch\n",
    "        å…¶ä»–åƒæ•¸èˆ‡åŸæœ¬ç›¸åŒ\n",
    "\n",
    "    è¿”å›:\n",
    "        results_df: åŒ…å«æ‰€æœ‰ fold çµæœçš„ DataFrame\n",
    "        all_predictions: æ‰€æœ‰ fold çš„é æ¸¬çµæœ\n",
    "        all_models: æ‰€æœ‰è¨“ç·´å¥½çš„æ¨¡å‹ (å¯é¸)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ”„ Walk-Forward Validation for MODWT-MoE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"ğŸ“Š Configuration:\")\n",
    "    print(f\"   Train Window: {train_window} days\")\n",
    "    print(f\"   Test Window: {test_window} days\")\n",
    "    print(f\"   Step Size: {step_size} days\")\n",
    "    print(f\"   Epochs per Fold: {num_epochs}\")\n",
    "\n",
    "    # è¨ˆç®—å¯ä»¥åšå¹¾å€‹ fold\n",
    "    total_len = len(df)\n",
    "    max_start = total_len - train_window - test_window\n",
    "    num_folds = max_start // step_size + 1\n",
    "\n",
    "    print(f\"   Total Data: {total_len} days\")\n",
    "    print(f\"   Number of Folds: {num_folds}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    all_results = []\n",
    "    all_predictions = []\n",
    "    all_models = []\n",
    "\n",
    "    for fold in range(num_folds):\n",
    "        fold_start_time = pd.Timestamp.now()\n",
    "\n",
    "        # è¨ˆç®—é€™å€‹ fold çš„æ™‚é–“ç¯„åœ\n",
    "        train_start = fold * step_size\n",
    "        train_end = train_start + train_window\n",
    "        test_end = train_end + test_window\n",
    "\n",
    "        # æª¢æŸ¥æ˜¯å¦è¶…å‡ºç¯„åœ\n",
    "        if test_end > total_len:\n",
    "            print(f\"\\nâš ï¸  Fold {fold+1}: Insufficient data, skipping...\")\n",
    "            break\n",
    "\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ğŸ”¹ Fold {fold+1}/{num_folds}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"   Train: Index {train_start:4d} to {train_end:4d} ({train_window} days)\")\n",
    "        print(f\"   Test:  Index {train_end:4d} to {test_end:4d} ({test_window} days)\")\n",
    "\n",
    "        # åˆ‡åˆ†æ•¸æ“š\n",
    "        fold_df = df.iloc[train_start:test_end].copy().reset_index(drop=True)\n",
    "\n",
    "        # æº–å‚™æ•¸æ“š (å…§éƒ¨æœƒå†æŒ‰ train_window/(train_window+test_window) åˆ‡åˆ†)\n",
    "        train_ratio = train_window / (train_window + test_window)\n",
    "\n",
    "        try:\n",
    "            train_loader, test_loader, scalers, components, energies = prepare_modwt_data(\n",
    "                fold_df,\n",
    "                vol_window=vol_window,\n",
    "                lookback=lookback,\n",
    "                forecast_horizon=forecast_horizon,\n",
    "                wavelet=wavelet,\n",
    "                level=level,\n",
    "                train_ratio=train_ratio,\n",
    "                batch_size=batch_size,\n",
    "                use_robust_scaler=use_robust_scaler\n",
    "            )\n",
    "\n",
    "            # è¨“ç·´æ¨¡å‹\n",
    "            print(f\"\\nğŸš€ Training Fold {fold+1}...\")\n",
    "            model, history, best_epoch = train_modwt_moe(\n",
    "                train_loader,\n",
    "                test_loader,\n",
    "                num_epochs=num_epochs,\n",
    "                lr=lr,\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "            # è©•ä¼°\n",
    "            print(f\"\\nğŸ“Š Evaluating Fold {fold+1}...\")\n",
    "            test_metrics, test_preds, test_targets, test_expert_preds, test_gating_weights = evaluate(\n",
    "                model, test_loader, device\n",
    "            )\n",
    "\n",
    "            # Inverse transform\n",
    "            target_scaler = scalers['target']\n",
    "            volatility_mean = scalers['volatility_mean']\n",
    "\n",
    "            test_preds_centered = target_scaler.inverse_transform(test_preds.reshape(-1, 1)).flatten()\n",
    "            test_targets_centered = target_scaler.inverse_transform(test_targets.reshape(-1, 1)).flatten()\n",
    "\n",
    "            test_preds_original = test_preds_centered + volatility_mean\n",
    "            test_targets_original = test_targets_centered + volatility_mean\n",
    "\n",
    "            rmse_original = np.sqrt(mean_squared_error(test_targets_original, test_preds_original))\n",
    "            mae_original = mean_absolute_error(test_targets_original, test_preds_original)\n",
    "            r2_original = r2_score(test_targets_original, test_preds_original)\n",
    "\n",
    "            # ä¿å­˜çµæœ\n",
    "            fold_result = {\n",
    "                'fold': fold + 1,\n",
    "                'train_start': train_start,\n",
    "                'train_end': train_end,\n",
    "                'test_start': train_end,\n",
    "                'test_end': test_end,\n",
    "                'best_epoch': best_epoch,\n",
    "                'rmse': rmse_original,\n",
    "                'mae': mae_original,\n",
    "                'r2': r2_original,\n",
    "                'direction_acc': test_metrics['direction_acc'],\n",
    "                'expert1_weight': test_gating_weights[:, 0].mean(),\n",
    "                'expert2_weight': test_gating_weights[:, 1].mean(),\n",
    "                'expert3_weight': test_gating_weights[:, 2].mean(),\n",
    "            }\n",
    "\n",
    "            all_results.append(fold_result)\n",
    "\n",
    "            # ä¿å­˜é æ¸¬çµæœ\n",
    "            all_predictions.append({\n",
    "                'fold': fold + 1,\n",
    "                'predictions': test_preds_original,\n",
    "                'targets': test_targets_original,\n",
    "                'gating_weights': test_gating_weights,\n",
    "                'expert_preds': test_expert_preds\n",
    "            })\n",
    "\n",
    "            # å¯é¸: ä¿å­˜æ¨¡å‹\n",
    "            all_models.append({\n",
    "                'fold': fold + 1,\n",
    "                'model': model.state_dict(),\n",
    "                'scalers': scalers\n",
    "            })\n",
    "\n",
    "            # æ‰“å°é€™å€‹ fold çš„çµæœ\n",
    "            print(f\"\\nâœ… Fold {fold+1} Results:\")\n",
    "            print(f\"   RMSE: {rmse_original:.4f}%\")\n",
    "            print(f\"   MAE: {mae_original:.4f}%\")\n",
    "            print(f\"   RÂ²: {r2_original:.6f}\")\n",
    "            print(f\"   Direction Accuracy: {test_metrics['direction_acc']*100:.2f}%\")\n",
    "            print(f\"   Gating Weights: E1={test_gating_weights[:, 0].mean():.3f}, \"\n",
    "                  f\"E2={test_gating_weights[:, 1].mean():.3f}, \"\n",
    "                  f\"E3={test_gating_weights[:, 2].mean():.3f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ Fold {fold+1} failed with error: {e}\")\n",
    "            continue\n",
    "\n",
    "    # åŒ¯ç¸½æ‰€æœ‰çµæœ\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ“Š Walk-Forward Validation Summary\")\n",
    "    print(\"=\" * 80)\n",
    "    print(results_df.to_string(index=False))\n",
    "\n",
    "    print(\"\\nğŸ“ˆ Statistical Summary:\")\n",
    "    print(f\"   RMSE:           {results_df['rmse'].mean():.4f}% Â± {results_df['rmse'].std():.4f}%\")\n",
    "    print(f\"   MAE:            {results_df['mae'].mean():.4f}% Â± {results_df['mae'].std():.4f}%\")\n",
    "    print(f\"   RÂ²:             {results_df['r2'].mean():.4f} Â± {results_df['r2'].std():.4f}\")\n",
    "    print(f\"   Direction Acc:  {results_df['direction_acc'].mean()*100:.2f}% Â± {results_df['direction_acc'].std()*100:.2f}%\")\n",
    "\n",
    "    print(\"\\nğŸ“Š Gating Weights Across Folds:\")\n",
    "    print(f\"   Expert 1 (Trend):    {results_df['expert1_weight'].mean():.3f} Â± {results_df['expert1_weight'].std():.3f}\")\n",
    "    print(f\"   Expert 2 (Cyclic):   {results_df['expert2_weight'].mean():.3f} Â± {results_df['expert2_weight'].std():.3f}\")\n",
    "    print(f\"   Expert 3 (High-Freq): {results_df['expert3_weight'].mean():.3f} Â± {results_df['expert3_weight'].std():.3f}\")\n",
    "\n",
    "    return results_df, all_predictions, all_models"
   ],
   "id": "ce0f17e8c4c822f5",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T07:58:09.829676Z",
     "start_time": "2025-11-13T07:58:09.823189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==================== Visualization Functions ====================\n",
    "def plot_training_history(history, save_path='training_history.png'):\n",
    "    \"\"\"Plot training history (Train/Test only)\"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    epochs = history['epochs']\n",
    "\n",
    "    # Loss curve\n",
    "    axes[0, 0].plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "    axes[0, 0].plot(epochs, history['test_loss'], 'r-', label='Test Loss (Monitor)', linewidth=2, alpha=0.7)\n",
    "    axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0, 0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0, 0].set_title('Training/Test Loss', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].legend(fontsize=11)\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "    # Train RMSE\n",
    "    axes[0, 1].plot(epochs, history['train_rmse'], 'purple', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0, 1].set_ylabel('RMSE', fontsize=12)\n",
    "    axes[0, 1].set_title('Training RMSE', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "    # Train MAE\n",
    "    axes[1, 0].plot(epochs, history['train_mae'], 'orange', linewidth=2)\n",
    "    axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('MAE', fontsize=12)\n",
    "    axes[1, 0].set_title('Training MAE', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "    # Log scale loss\n",
    "    axes[1, 1].plot(epochs, history['train_loss'], 'b-', label='Train', linewidth=2)\n",
    "    axes[1, 1].plot(epochs, history['test_loss'], 'r-', label='Test (Monitor)', linewidth=2, alpha=0.7)\n",
    "    axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Loss', fontsize=12)\n",
    "    axes[1, 1].set_title('Loss Comparison (Log Scale)', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_yscale('log')\n",
    "    axes[1, 1].legend(fontsize=11)\n",
    "    axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"ğŸ“Š Saved: {save_path}\")\n",
    "    plt.close()"
   ],
   "id": "fe510b48a37a03ab",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T07:58:09.838752Z",
     "start_time": "2025-11-13T07:58:09.833434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_predictions(true_values, predictions, save_path='predictions.png'):\n",
    "    \"\"\"Plot predictions vs actual\"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "    axes[0].plot(true_values, label='Actual', color='blue', linewidth=1.5, alpha=0.7)\n",
    "    axes[0].plot(predictions, label='Predicted', color='red', linewidth=1.5, alpha=0.7)\n",
    "    axes[0].set_xlabel('Time', fontsize=12)\n",
    "    axes[0].set_ylabel('Volatility (%)', fontsize=12)\n",
    "    axes[0].set_title('Volatility Predictions vs Actual', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend(fontsize=11)\n",
    "    axes[0].grid(alpha=0.3)\n",
    "\n",
    "    axes[1].scatter(true_values, predictions, alpha=0.5, s=20)\n",
    "    axes[1].plot([true_values.min(), true_values.max()],\n",
    "                 [true_values.min(), true_values.max()],\n",
    "                 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    axes[1].set_xlabel('Actual Volatility (%)', fontsize=12)\n",
    "    axes[1].set_ylabel('Predicted Volatility (%)', fontsize=12)\n",
    "    axes[1].set_title('Prediction Scatter Plot', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(fontsize=11)\n",
    "    axes[1].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"ğŸ“Š Saved: {save_path}\")\n",
    "    plt.close()"
   ],
   "id": "18c4540e475ab4e9",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T07:58:09.854828Z",
     "start_time": "2025-11-13T07:58:09.849896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_gating_weights(gating_weights, save_path='gating_weights.png'):\n",
    "    \"\"\"Plot gating weights over time\"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "    time = np.arange(len(gating_weights))\n",
    "\n",
    "    axes[0].fill_between(time, 0, gating_weights[:, 0],\n",
    "                         label='Expert 1 (Trend)', color='green', alpha=0.6)\n",
    "    axes[0].fill_between(time, gating_weights[:, 0],\n",
    "                         gating_weights[:, 0] + gating_weights[:, 1],\n",
    "                         label='Expert 2 (Cyclic)', color='blue', alpha=0.6)\n",
    "    axes[0].fill_between(time, gating_weights[:, 0] + gating_weights[:, 1],\n",
    "                         gating_weights[:, 0] + gating_weights[:, 1] + gating_weights[:, 2],\n",
    "                         label='Expert 3 (High-Freq)', color='orange', alpha=0.6)\n",
    "    axes[0].set_xlabel('Time', fontsize=12)\n",
    "    axes[0].set_ylabel('Gating Weight', fontsize=12)\n",
    "    axes[0].set_title('Gating Weights Over Time (Stacked)', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend(fontsize=11)\n",
    "    axes[0].set_ylim([0, 1])\n",
    "    axes[0].grid(alpha=0.3)\n",
    "\n",
    "    axes[1].plot(time, gating_weights[:, 0], label='Expert 1 (Trend)',\n",
    "                 color='green', linewidth=1.5, alpha=0.8)\n",
    "    axes[1].plot(time, gating_weights[:, 1], label='Expert 2 (Cyclic)',\n",
    "                 color='blue', linewidth=1.5, alpha=0.8)\n",
    "    axes[1].plot(time, gating_weights[:, 2], label='Expert 3 (High-Freq)',\n",
    "                 color='orange', linewidth=1.5, alpha=0.8)\n",
    "    axes[1].set_xlabel('Time', fontsize=12)\n",
    "    axes[1].set_ylabel('Gating Weight', fontsize=12)\n",
    "    axes[1].set_title('Individual Gating Weights Over Time', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(fontsize=11)\n",
    "    axes[1].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"ğŸ“Š Saved: {save_path}\")\n",
    "    plt.close()"
   ],
   "id": "566213cb91ce2fc0",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T07:58:09.862466Z",
     "start_time": "2025-11-13T07:58:09.859337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_results_to_csv(targets, predictions, gating_weights, expert_preds, scalers, save_path):\n",
    "    \"\"\"Save results to CSV\"\"\"\n",
    "\n",
    "    target_scaler = scalers['target']\n",
    "    volatility_mean = scalers['volatility_mean']\n",
    "\n",
    "    targets_centered = target_scaler.inverse_transform(targets.reshape(-1, 1)).flatten()\n",
    "    predictions_centered = target_scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # åŠ å›å‡å€¼\n",
    "    targets_original = targets_centered + volatility_mean\n",
    "    predictions_original = predictions_centered + volatility_mean\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        'True_Volatility': targets_original,\n",
    "        'Predicted_Volatility': predictions_original,\n",
    "        'Expert1_Weight': gating_weights[:, 0],\n",
    "        'Expert2_Weight': gating_weights[:, 1],\n",
    "        'Expert3_Weight': gating_weights[:, 2],\n",
    "        'Expert1_Pred': expert_preds[:, 0],\n",
    "        'Expert2_Pred': expert_preds[:, 1],\n",
    "        'Expert3_Pred': expert_preds[:, 2],\n",
    "    })\n",
    "\n",
    "    results_df.to_csv(save_path, index=False)\n",
    "    print(f\"ğŸ’¾ Results saved to {save_path}\")\n",
    "\n",
    "    return results_df"
   ],
   "id": "ffe35ee980f16804",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T07:58:09.872116Z",
     "start_time": "2025-11-13T07:58:09.867860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def analyze_gating_dynamics(gating_weights, volatility):\n",
    "    \"\"\"åˆ†æ Gating å‹•æ…‹\"\"\"\n",
    "\n",
    "    low_vol = volatility < np.percentile(volatility, 33)\n",
    "    mid_vol = (volatility >= np.percentile(volatility, 33)) & (volatility <= np.percentile(volatility, 67))\n",
    "    high_vol = volatility > np.percentile(volatility, 67)\n",
    "\n",
    "    print(\"ğŸ“Š Gating Weights by Volatility Regime:\")\n",
    "    print(\"\\nLow Volatility:\")\n",
    "    print(f\"  Expert 1 (Trend): {gating_weights[low_vol, 0].mean():.3f}\")\n",
    "    print(f\"  Expert 2 (Cyclic): {gating_weights[low_vol, 1].mean():.3f}\")\n",
    "    print(f\"  Expert 3 (High-Freq): {gating_weights[low_vol, 2].mean():.3f}\")\n",
    "\n",
    "    print(\"\\nMedium Volatility:\")\n",
    "    print(f\"  Expert 1 (Trend): {gating_weights[mid_vol, 0].mean():.3f}\")\n",
    "    print(f\"  Expert 2 (Cyclic): {gating_weights[mid_vol, 1].mean():.3f}\")\n",
    "    print(f\"  Expert 3 (High-Freq): {gating_weights[mid_vol, 2].mean():.3f}\")\n",
    "\n",
    "    print(\"\\nHigh Volatility:\")\n",
    "    print(f\"  Expert 1 (Trend): {gating_weights[high_vol, 0].mean():.3f}\")\n",
    "    print(f\"  Expert 2 (Cyclic): {gating_weights[high_vol, 1].mean():.3f}\")\n",
    "    print(f\"  Expert 3 (High-Freq): {gating_weights[high_vol, 2].mean():.3f}\")"
   ],
   "id": "529302ebdf749a82",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T07:58:09.883161Z",
     "start_time": "2025-11-13T07:58:09.875864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_gating_by_regime(gating_weights, targets_original, save_path='gating_dynamics_by_regime.png'):\n",
    "    \"\"\"ç•«å‡ºä¸åŒæ³¢å‹•å€åˆ¶ä¸‹çš„ Gating æ¬Šé‡\"\"\"\n",
    "\n",
    "    low_vol = targets_original < np.percentile(targets_original, 33)\n",
    "    mid_vol = (targets_original >= np.percentile(targets_original, 33)) & \\\n",
    "              (targets_original <= np.percentile(targets_original, 67))\n",
    "    high_vol = targets_original > np.percentile(targets_original, 67)\n",
    "\n",
    "    regimes = ['Low\\nVolatility', 'Medium\\nVolatility', 'High\\nVolatility']\n",
    "    expert1_means = [\n",
    "        gating_weights[low_vol, 0].mean(),\n",
    "        gating_weights[mid_vol, 0].mean(),\n",
    "        gating_weights[high_vol, 0].mean()\n",
    "    ]\n",
    "    expert2_means = [\n",
    "        gating_weights[low_vol, 1].mean(),\n",
    "        gating_weights[mid_vol, 1].mean(),\n",
    "        gating_weights[high_vol, 1].mean()\n",
    "    ]\n",
    "    expert3_means = [\n",
    "        gating_weights[low_vol, 2].mean(),\n",
    "        gating_weights[mid_vol, 2].mean(),\n",
    "        gating_weights[high_vol, 2].mean()\n",
    "    ]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    x = np.arange(len(regimes))\n",
    "    width = 0.6\n",
    "\n",
    "    p1 = axes[0].bar(x, expert1_means, width, label='Expert 1 (Trend)', color='green', alpha=0.8)\n",
    "    p2 = axes[0].bar(x, expert2_means, width, bottom=expert1_means,\n",
    "                     label='Expert 2 (Cyclic)', color='blue', alpha=0.8)\n",
    "    p3 = axes[0].bar(x, expert3_means, width,\n",
    "                     bottom=np.array(expert1_means) + np.array(expert2_means),\n",
    "                     label='Expert 3 (High-Freq)', color='orange', alpha=0.8)\n",
    "\n",
    "    axes[0].set_ylabel('Gating Weight', fontsize=12)\n",
    "    axes[0].set_title('Gating Weights by Volatility Regime (Stacked)',\n",
    "                      fontsize=13, fontweight='bold')\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(regimes)\n",
    "    axes[0].legend(loc='upper left', fontsize=10)\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    axes[0].set_ylim([0, 1])\n",
    "\n",
    "    for i, (e1, e2, e3) in enumerate(zip(expert1_means, expert2_means, expert3_means)):\n",
    "        axes[0].text(i, e1/2, f'{e1:.1%}', ha='center', va='center',\n",
    "                     fontweight='bold', color='white', fontsize=10)\n",
    "        axes[0].text(i, e1 + e2/2, f'{e2:.1%}', ha='center', va='center',\n",
    "                     fontweight='bold', color='white', fontsize=10)\n",
    "        axes[0].text(i, e1 + e2 + e3/2, f'{e3:.1%}', ha='center', va='center',\n",
    "                     fontweight='bold', color='white', fontsize=10)\n",
    "\n",
    "    axes[1].plot(regimes, expert1_means, marker='o', linewidth=2.5,\n",
    "                 markersize=10, label='Expert 1 (Trend)', color='green')\n",
    "    axes[1].plot(regimes, expert2_means, marker='s', linewidth=2.5,\n",
    "                 markersize=10, label='Expert 2 (Cyclic)', color='blue')\n",
    "    axes[1].plot(regimes, expert3_means, marker='^', linewidth=2.5,\n",
    "                 markersize=10, label='Expert 3 (High-Freq)', color='orange')\n",
    "\n",
    "    axes[1].set_ylabel('Gating Weight', fontsize=12)\n",
    "    axes[1].set_title('Gating Weight Dynamics Across Regimes',\n",
    "                      fontsize=13, fontweight='bold')\n",
    "    axes[1].legend(fontsize=10)\n",
    "    axes[1].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"ğŸ“Š Saved: {save_path}\")\n",
    "    plt.close()"
   ],
   "id": "501f61f18fef3ec6",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T07:58:09.894907Z",
     "start_time": "2025-11-13T07:58:09.886613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_wfv_results(results_df, save_path='../results/wfv_summary.png'):\n",
    "    \"\"\"è¦–è¦ºåŒ– Walk-Forward Validation çµæœ\"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "    folds = results_df['fold']\n",
    "\n",
    "    # RMSE è¶¨å‹¢\n",
    "    axes[0, 0].plot(folds, results_df['rmse'], marker='o', linewidth=2, markersize=8)\n",
    "    axes[0, 0].axhline(results_df['rmse'].mean(), color='r', linestyle='--',\n",
    "                       label=f\"Mean: {results_df['rmse'].mean():.4f}%\")\n",
    "    axes[0, 0].set_xlabel('Fold', fontsize=12)\n",
    "    axes[0, 0].set_ylabel('RMSE (%)', fontsize=12)\n",
    "    axes[0, 0].set_title('RMSE Across Folds', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "    # MAE è¶¨å‹¢\n",
    "    axes[0, 1].plot(folds, results_df['mae'], marker='s', linewidth=2,\n",
    "                    markersize=8, color='orange')\n",
    "    axes[0, 1].axhline(results_df['mae'].mean(), color='r', linestyle='--',\n",
    "                       label=f\"Mean: {results_df['mae'].mean():.4f}%\")\n",
    "    axes[0, 1].set_xlabel('Fold', fontsize=12)\n",
    "    axes[0, 1].set_ylabel('MAE (%)', fontsize=12)\n",
    "    axes[0, 1].set_title('MAE Across Folds', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "    # RÂ² è¶¨å‹¢\n",
    "    axes[0, 2].plot(folds, results_df['r2'], marker='^', linewidth=2,\n",
    "                    markersize=8, color='green')\n",
    "    axes[0, 2].axhline(results_df['r2'].mean(), color='r', linestyle='--',\n",
    "                       label=f\"Mean: {results_df['r2'].mean():.4f}\")\n",
    "    axes[0, 2].set_xlabel('Fold', fontsize=12)\n",
    "    axes[0, 2].set_ylabel('RÂ²', fontsize=12)\n",
    "    axes[0, 2].set_title('RÂ² Across Folds', fontsize=14, fontweight='bold')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(alpha=0.3)\n",
    "\n",
    "    # Direction Accuracy\n",
    "    axes[1, 0].plot(folds, results_df['direction_acc']*100, marker='d',\n",
    "                    linewidth=2, markersize=8, color='purple')\n",
    "    axes[1, 0].axhline(results_df['direction_acc'].mean()*100, color='r',\n",
    "                       linestyle='--', label=f\"Mean: {results_df['direction_acc'].mean()*100:.2f}%\")\n",
    "    axes[1, 0].set_xlabel('Fold', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Direction Accuracy (%)', fontsize=12)\n",
    "    axes[1, 0].set_title('Direction Accuracy Across Folds', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "    # Gating Weights\n",
    "    axes[1, 1].plot(folds, results_df['expert1_weight'], marker='o',\n",
    "                    linewidth=2, label='Expert 1 (Trend)', color='green')\n",
    "    axes[1, 1].plot(folds, results_df['expert2_weight'], marker='s',\n",
    "                    linewidth=2, label='Expert 2 (Cyclic)', color='blue')\n",
    "    axes[1, 1].plot(folds, results_df['expert3_weight'], marker='^',\n",
    "                    linewidth=2, label='Expert 3 (High-Freq)', color='orange')\n",
    "    axes[1, 1].set_xlabel('Fold', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Average Gating Weight', fontsize=12)\n",
    "    axes[1, 1].set_title('Gating Weights Across Folds', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "    # Box plot for RMSE\n",
    "    bp = axes[1, 2].boxplot([results_df['rmse']], labels=['RMSE'],\n",
    "                            patch_artist=True, widths=0.5)\n",
    "    bp['boxes'][0].set_facecolor('lightblue')\n",
    "    axes[1, 2].set_ylabel('RMSE (%)', fontsize=12)\n",
    "    axes[1, 2].set_title('RMSE Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[1, 2].grid(alpha=0.3, axis='y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"ğŸ“Š Saved: {save_path}\")\n",
    "    plt.close()"
   ],
   "id": "5c599f2f8e6fc086",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T07:58:09.902371Z",
     "start_time": "2025-11-13T07:58:09.898230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_wfv_predictions(all_predictions, save_path='../results/wfv_predictions.png'):\n",
    "    \"\"\"è¦–è¦ºåŒ–æ‰€æœ‰ fold çš„é æ¸¬çµæœ\"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(len(all_predictions), 1,\n",
    "                             figsize=(16, 4*len(all_predictions)))\n",
    "\n",
    "    if len(all_predictions) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, pred_data in enumerate(all_predictions):\n",
    "        fold = pred_data['fold']\n",
    "        preds = pred_data['predictions']\n",
    "        targets = pred_data['targets']\n",
    "\n",
    "        axes[i].plot(targets, label='Actual', color='blue', linewidth=1.5, alpha=0.7)\n",
    "        axes[i].plot(preds, label='Predicted', color='red', linewidth=1.5, alpha=0.7)\n",
    "        axes[i].set_xlabel('Time Step', fontsize=11)\n",
    "        axes[i].set_ylabel('Volatility (%)', fontsize=11)\n",
    "        axes[i].set_title(f'Fold {fold} Predictions', fontsize=13, fontweight='bold')\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"ğŸ“Š Saved: {save_path}\")\n",
    "    plt.close()"
   ],
   "id": "42c13b26293d7866",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T07:58:10.036940Z",
     "start_time": "2025-11-13T07:58:09.905573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "print(\"\\nğŸ“‚ Loading data...\")\n",
    "df = pd.read_csv(\"../dataset/USD_TWD.csv\")\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df = df.sort_values(\"Date\").reset_index(drop=True)\n",
    "print(f\"âœ… Loaded {len(df)} days of data\")"
   ],
   "id": "9b10a8f9807ddd87",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‚ Loading data...\n",
      "âœ… Loaded 6738 days of data\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T07:58:10.446248Z",
     "start_time": "2025-11-13T07:58:10.060728Z"
    }
   },
   "cell_type": "code",
   "source": [
    " # Prepare data with centering (80/20 split)\n",
    "train_loader, test_loader, scalers, components, energies = prepare_modwt_data(\n",
    "    df,\n",
    "    vol_window=7,\n",
    "    lookback=30,\n",
    "    forecast_horizon=1,\n",
    "    wavelet='haar',\n",
    "    level=4,\n",
    "    train_ratio=0.80,  # 80/20 åˆ‡åˆ†\n",
    "    use_robust_scaler=False\n",
    ")"
   ],
   "id": "7442c0c8d98ae043",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ”§ Data Preparation with MODWT + Centering (Train/Test Split)\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Step 1: Calculate volatility...\n",
      "   âœ… Volatility calculated: 6731 samples\n",
      "      Mean: 3.9496%\n",
      "      Std: 2.6214%\n",
      "      Range: [0.0000, 46.8188]\n",
      "\n",
      "ğŸ“Š Step 2: Time-series split (80% Train / 20% Test)...\n",
      "   Total samples: 6731\n",
      "   Train: 0 to 5384 (5384 samples, 80%)\n",
      "   Test: 5384 to 6731 (1347 samples, 20%)\n",
      "\n",
      "ğŸ“Š Step 3: Centering (remove mean)...\n",
      "   âœ… Train mean: 3.7850% (will be subtracted)\n",
      "      Train centered: mean=-0.000000, std=2.3078\n",
      "      Test centered: mean=0.822364, std=3.5368\n",
      "\n",
      "ğŸ“Š Step 4: Separate MODWT decomposition with MRA...\n",
      "\n",
      "   ğŸ”¹ Decomposing TRAIN set...\n",
      "   Decomposing with modwtpy (wavelet=haar, level=4, MRA=True)...\n",
      "   âœ… MODWT Decomposition Complete\n",
      "      Wavelet: haar, Level: 4\n",
      "      Components: ['cD1', 'cD2', 'cD3', 'cD4', 'cA4_trend']\n",
      "      Signal length: 5384\n",
      "      Reconstruction Error: 0.0000000000\n",
      "\n",
      "   ğŸ”¹ Decomposing TEST set...\n",
      "   Decomposing with modwtpy (wavelet=haar, level=4, MRA=True)...\n",
      "   âœ… MODWT Decomposition Complete\n",
      "      Wavelet: haar, Level: 4\n",
      "      Components: ['cD1', 'cD2', 'cD3', 'cD4', 'cA4_trend']\n",
      "      Signal length: 1347\n",
      "      Reconstruction Error: 0.0000000000\n",
      "\n",
      "ğŸ“Š Component Energies (Train set - After Centering):\n",
      "   cD1: 2.81%\n",
      "   cD2: 2.59%\n",
      "   cD3: 5.84%\n",
      "   cD4: 9.59%\n",
      "   cA4_trend: 79.16%\n",
      "\n",
      "ğŸ“Š Step 5: Scaling components...\n",
      "   Using StandardScaler\n",
      "   âœ… cD1: Mean=-0.000000, Std=0.321622\n",
      "   âœ… cD2: Mean=0.000000, Std=0.308675\n",
      "   âœ… cD3: Mean=-0.000000, Std=0.463764\n",
      "   âœ… cD4: Mean=-0.000000, Std=0.594181\n",
      "   âœ… cA4_trend: Mean=-0.000000, Std=1.706818\n",
      "\n",
      "ğŸ“Š Step 6: Create datasets...\n",
      "   âœ… Dataset Created: 5354 samples\n",
      "   âœ… Dataset Created: 1317 samples\n",
      "\n",
      "âœ… Data preparation complete!\n",
      "   Train batches: 168\n",
      "   Test batches: 42\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T07:59:12.385655Z",
     "start_time": "2025-11-13T07:58:10.462485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "    # Train model (ç„¡ validation)\n",
    "trained_model, training_history, best_epoch = train_modwt_moe(\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    num_epochs=50,\n",
    "    lr=0.001,\n",
    "    device=DEVICE\n",
    ")"
   ],
   "id": "78bfbc581c1056c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸš€ Training MODWT-MoE Model (80/20 Split)\n",
      "================================================================================\n",
      "Epoch   1/50 | Train Loss: 0.1319 | Test Loss: 0.1339 | Train RMSE: 0.5570\n",
      "   âœ… New best model! Train Loss: 0.1319\n",
      "Epoch   5/50 | Train Loss: 0.0403 | Test Loss: 0.0840 | Train RMSE: 0.2899\n",
      "   âœ… New best model! Train Loss: 0.0403\n",
      "Epoch  10/50 | Train Loss: 0.0331 | Test Loss: 0.0792 | Train RMSE: 0.2606\n",
      "   âœ… New best model! Train Loss: 0.0331\n",
      "Epoch  15/50 | Train Loss: 0.0283 | Test Loss: 0.0767 | Train RMSE: 0.2395\n",
      "Epoch  20/50 | Train Loss: 0.0272 | Test Loss: 0.0763 | Train RMSE: 0.2348\n",
      "Epoch  25/50 | Train Loss: 0.0235 | Test Loss: 0.0787 | Train RMSE: 0.2186\n",
      "Epoch  30/50 | Train Loss: 0.0217 | Test Loss: 0.0770 | Train RMSE: 0.2096\n",
      "   âœ… New best model! Train Loss: 0.0217\n",
      "Epoch  35/50 | Train Loss: 0.0211 | Test Loss: 0.0714 | Train RMSE: 0.2063\n",
      "Epoch  40/50 | Train Loss: 0.0206 | Test Loss: 0.0689 | Train RMSE: 0.2043\n",
      "Epoch  45/50 | Train Loss: 0.0195 | Test Loss: 0.0694 | Train RMSE: 0.1981\n",
      "Epoch  50/50 | Train Loss: 0.0171 | Test Loss: 0.0707 | Train RMSE: 0.1861\n",
      "\n",
      "âœ… Training complete! Best model from epoch 46\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T07:59:15.625515Z",
     "start_time": "2025-11-13T07:59:12.453651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate on test set\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“Š Final Evaluation on Test Set\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_metrics, test_preds, test_targets, test_expert_preds, test_gating_weights = evaluate(\n",
    "    trained_model, test_loader, DEVICE\n",
    ")\n",
    "\n",
    "# Inverse transform (çµ±ä¸€ç”¨ inverse_transform)\n",
    "target_scaler = scalers['target']\n",
    "volatility_mean = scalers['volatility_mean']\n",
    "\n",
    "# æ­¥é©Ÿ1: inverse_transform\n",
    "test_preds_centered = target_scaler.inverse_transform(test_preds.reshape(-1, 1)).flatten()\n",
    "test_targets_centered = target_scaler.inverse_transform(test_targets.reshape(-1, 1)).flatten()\n",
    "\n",
    "# æ­¥é©Ÿ2: åŠ å›å‡å€¼\n",
    "test_preds_original = test_preds_centered + volatility_mean\n",
    "test_targets_original = test_targets_centered + volatility_mean\n",
    "\n",
    "rmse_original = np.sqrt(mean_squared_error(test_targets_original, test_preds_original))\n",
    "mae_original = mean_absolute_error(test_targets_original, test_preds_original)\n",
    "r2_original = r2_score(test_targets_original, test_preds_original)\n",
    "\n",
    "print(f\"\\nâœ… Test Set Performance (Original Scale):\")\n",
    "print(f\"   RMSE: {rmse_original:.4f}%\")\n",
    "print(f\"   MAE: {mae_original:.4f}%\")\n",
    "print(f\"   RÂ²: {r2_original:.6f}\")\n",
    "print(f\"   Direction Accuracy: {test_metrics['direction_acc']:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Gating Weights on Test Set:\")\n",
    "print(f\"   Expert 1 (Trend): {test_gating_weights[:, 0].mean():.3f} Â± {test_gating_weights[:, 0].std():.3f}\")\n",
    "print(f\"   Expert 2 (Cyclic): {test_gating_weights[:, 1].mean():.3f} Â± {test_gating_weights[:, 1].std():.3f}\")\n",
    "print(f\"   Expert 3 (High-Freq): {test_gating_weights[:, 2].mean():.3f} Â± {test_gating_weights[:, 2].std():.3f}\")\n",
    "\n",
    "# Visualizations\n",
    "print(\"\\nğŸ“Š Generating visualizations...\")\n",
    "plot_training_history(training_history, '../results/training_history_8020.png')\n",
    "plot_predictions(test_targets_original, test_preds_original, '../results/test_predictions_8020.png')\n",
    "plot_gating_weights(test_gating_weights, '../results/test_gating_weights_8020.png')\n",
    "\n",
    "# Save results\n",
    "test_results_df = save_results_to_csv(\n",
    "    test_targets, test_preds, test_gating_weights,\n",
    "    test_expert_preds, scalers, '../results/test_results_8020.csv'\n",
    ")\n",
    "\n",
    "# Analysis\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“Š Gating Dynamics Analysis (Test Set)\")\n",
    "print(\"=\" * 80)\n",
    "analyze_gating_dynamics(test_gating_weights, test_targets_original)\n",
    "plot_gating_by_regime(test_gating_weights, test_targets_original,\n",
    "                      '../results/test_gating_dynamics_by_regime_8020.png')\n",
    "\n",
    "print(\"\\nâœ… All done! Check the ../results/ folder for outputs.\")\n",
    "print(f\"\\nğŸ† Best model from epoch {best_epoch}\")\n",
    "print(f\"   Final Test RMSE: {rmse_original:.4f}%\")\n",
    "print(f\"   Final Test MAE: {mae_original:.4f}%\")\n",
    "print(f\"   Final R2: {r2_original:.4f}\")\n",
    "print(f\"   Final Test Direction Accuracy: {test_metrics['direction_acc']*100:.2f}%\")\n",
    "\n",
    "# Summary of energy distribution\n",
    "print(f\"\\nğŸ“Š Component Energy Distribution (After Centering):\")\n",
    "for name, energy in energies.items():\n",
    "    print(f\"   {name}: {energy:.2f}%\")"
   ],
   "id": "ec1bafa4a2c647ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ“Š Final Evaluation on Test Set\n",
      "================================================================================\n",
      "\n",
      "âœ… Test Set Performance (Original Scale):\n",
      "   RMSE: 1.9260%\n",
      "   MAE: 0.3282%\n",
      "   RÂ²: 0.707079\n",
      "   Direction Accuracy: 0.8822\n",
      "\n",
      "ğŸ“Š Gating Weights on Test Set:\n",
      "   Expert 1 (Trend): 0.353 Â± 0.159\n",
      "   Expert 2 (Cyclic): 0.252 Â± 0.172\n",
      "   Expert 3 (High-Freq): 0.395 Â± 0.090\n",
      "\n",
      "ğŸ“Š Generating visualizations...\n",
      "ğŸ“Š Saved: ../results/training_history_8020.png\n",
      "ğŸ“Š Saved: ../results/test_predictions_8020.png\n",
      "ğŸ“Š Saved: ../results/test_gating_weights_8020.png\n",
      "ğŸ’¾ Results saved to ../results/test_results_8020.csv\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š Gating Dynamics Analysis (Test Set)\n",
      "================================================================================\n",
      "ğŸ“Š Gating Weights by Volatility Regime:\n",
      "\n",
      "Low Volatility:\n",
      "  Expert 1 (Trend): 0.220\n",
      "  Expert 2 (Cyclic): 0.432\n",
      "  Expert 3 (High-Freq): 0.348\n",
      "\n",
      "Medium Volatility:\n",
      "  Expert 1 (Trend): 0.346\n",
      "  Expert 2 (Cyclic): 0.230\n",
      "  Expert 3 (High-Freq): 0.424\n",
      "\n",
      "High Volatility:\n",
      "  Expert 1 (Trend): 0.495\n",
      "  Expert 2 (Cyclic): 0.094\n",
      "  Expert 3 (High-Freq): 0.411\n",
      "ğŸ“Š Saved: ../results/test_gating_dynamics_by_regime_8020.png\n",
      "\n",
      "âœ… All done! Check the ../results/ folder for outputs.\n",
      "\n",
      "ğŸ† Best model from epoch 46\n",
      "   Final Test RMSE: 1.9260%\n",
      "   Final Test MAE: 0.3282%\n",
      "   Final R2: 0.7071\n",
      "   Final Test Direction Accuracy: 88.22%\n",
      "\n",
      "ğŸ“Š Component Energy Distribution (After Centering):\n",
      "   cD1: 2.81%\n",
      "   cD2: 2.59%\n",
      "   cD3: 5.84%\n",
      "   cD4: 9.59%\n",
      "   cA4_trend: 79.16%\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T07:59:15.644177Z",
     "start_time": "2025-11-13T07:59:15.641410Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "72e91bc0dadb6290",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
